{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e3906a-5a09-4a72-bcfd-3d350753665e",
   "metadata": {},
   "source": [
    "# NYC 311 Capstone - Data Cleaning \n",
    "## Stephen Behunin | BrainStation | stephenbehuninwork@gmail.com\n",
    "__________________________________________________________________________________________________________________________________________________________\n",
    "## Notebook Executive Summary\n",
    "__________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "The goal of this section is cleaning and formatting the raw data provided by the NYC 311 Data Portal for use in analysis and modeling. Areas of primary concern are missing/invalid data, formatting column datatypes, and filtering the data to the scope of the analysis. \n",
    "\n",
    "## Background\n",
    "_________________________________________________________________________________________________________________________________________________________\n",
    "The cleaning in this notebook is done in two stages, first the dataset is cleaned on complaint relevant data to ensure that each SR is valid and will be stored in a .csv file. \n",
    "Next the entire dataset is condensed into a timeseries format and converted into another .csv file.\n",
    "\n",
    "## Layout\n",
    "__________________________________________________________________________________________________________________________________________________________\n",
    "#### Pre-amble\n",
    "- Importing packages\n",
    "- Reading in the source data\n",
    "- Creating a backup copy of the original dataset\n",
    "\n",
    "#### Part 1 - Dataset Basics\n",
    "- Collecting basic information about the dataset\n",
    "\n",
    "#### Part 2 - Columns\n",
    "- Dropping unwanted columns\n",
    "- Top Ten Descriptors\n",
    "- Formatting column datatypes\n",
    "\n",
    "#### Part 3 - Rows\n",
    "- Dealing with missing or null data\n",
    "- Searching for duplicate values\n",
    "- Resetting the index column\n",
    "\n",
    "#### Part 4 - Limiting Scope\n",
    "- Filtering out unclosed complaints\n",
    "- Restricting the dataset to complaints from 2010 to 2019\n",
    "- Converting the complaint dataframe to a .csv file\n",
    "\n",
    "#### Part 5- Creating the Timeseries Dataframe\n",
    "- Creating the dataset\n",
    "- Cleaning the new dataset\n",
    "\n",
    "#### Part 6- Conversion to CSV\n",
    "- Final dimensions check\n",
    "- Converting the dataframe to a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6f5aa-cdce-4a15-a753-89db5f514775",
   "metadata": {},
   "source": [
    "## Pre-amble\n",
    "__________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1158409-19d9-4089-9b6b-59d9d0ac12b7",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec57fd47-80b4-423e-8317-457cc3daf203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas and Numpy packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Starting notebook timer\n",
    "e_start = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2f33d-6fff-4a0c-b7ff-039040c071fd",
   "metadata": {},
   "source": [
    "### Reading in data\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdebb9f-3940-4c08-a357-96dbaacecf45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\anaconda3\\envs\\testing_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (8,17,20,31,32,33,34,35,36,37) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:35.774966\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Reading in the data\n",
    "nyc_311 = pd.read_csv(\"311 Data/311_Data.csv\")\n",
    "\n",
    "# Displaying the dataframe\n",
    "nyc_311.head(5)\n",
    "finish = datetime.now()\n",
    "duration = finish - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01613600-f152-4301-938a-971f311defbf",
   "metadata": {},
   "source": [
    "### Creating a copy of the original data \n",
    "________________________________________\n",
    "This copy of the data is included for the readers convenience to avoid rerunning the notebook if a restoration of the original data is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d8bff04-237e-4923-86a6-12dad7f4df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the backup copy to the original dataset\n",
    "nyc_311_original = nyc_311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe1c49-1012-4361-a146-73852384a70a",
   "metadata": {},
   "source": [
    "Code to implement a reset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92df2c16-f83b-48b0-aff9-6a5439dbfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will reset the data to its original state\n",
    "nyc_311 = nyc_311_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876e3a8-4d5e-4d8d-ab83-f1eba583dd83",
   "metadata": {},
   "source": [
    "## Part 1- Dataset Basics\n",
    "_____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafabffb-a5e2-4a8b-9dac-cc20f4bfcce4",
   "metadata": {},
   "source": [
    "### Gathering basic information on the dataset\n",
    "_______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9630faf-600c-4d3e-9a2d-a598d9ae31fb",
   "metadata": {},
   "source": [
    "Checking shape of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312f5b91-6069-4505-954b-727b476b9334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NYC 311 dataframe has 26256834 rows and 41 columns.\n"
     ]
    }
   ],
   "source": [
    "shape = nyc_311.shape\n",
    "rows = shape[0]\n",
    "columns = shape[1]\n",
    "print(f\"The NYC 311 dataframe has {rows} rows and {columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e31cfa-1c5a-4d01-8358-5d37b6d0463b",
   "metadata": {},
   "source": [
    "Checking column names and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faba8885-d922-4416-88d5-b677ab82eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26256834 entries, 0 to 26256833\n",
      "Data columns (total 41 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   Unique Key                      int64  \n",
      " 1   Created Date                    object \n",
      " 2   Closed Date                     object \n",
      " 3   Agency                          object \n",
      " 4   Agency Name                     object \n",
      " 5   Complaint Type                  object \n",
      " 6   Descriptor                      object \n",
      " 7   Location Type                   object \n",
      " 8   Incident Zip                    object \n",
      " 9   Incident Address                object \n",
      " 10  Street Name                     object \n",
      " 11  Cross Street 1                  object \n",
      " 12  Cross Street 2                  object \n",
      " 13  Intersection Street 1           object \n",
      " 14  Intersection Street 2           object \n",
      " 15  Address Type                    object \n",
      " 16  City                            object \n",
      " 17  Landmark                        object \n",
      " 18  Facility Type                   object \n",
      " 19  Status                          object \n",
      " 20  Due Date                        object \n",
      " 21  Resolution Description          object \n",
      " 22  Resolution Action Updated Date  object \n",
      " 23  Community Board                 object \n",
      " 24  BBL                             float64\n",
      " 25  Borough                         object \n",
      " 26  X Coordinate (State Plane)      float64\n",
      " 27  Y Coordinate (State Plane)      float64\n",
      " 28  Open Data Channel Type          object \n",
      " 29  Park Facility Name              object \n",
      " 30  Park Borough                    object \n",
      " 31  Vehicle Type                    object \n",
      " 32  Taxi Company Borough            object \n",
      " 33  Taxi Pick Up Location           object \n",
      " 34  Bridge Highway Name             object \n",
      " 35  Bridge Highway Direction        object \n",
      " 36  Road Ramp                       object \n",
      " 37  Bridge Highway Segment          object \n",
      " 38  Latitude                        float64\n",
      " 39  Longitude                       float64\n",
      " 40  Location                        object \n",
      "dtypes: float64(5), int64(1), object(35)\n",
      "memory usage: 8.0+ GB\n"
     ]
    }
   ],
   "source": [
    "nyc_311.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8410aa59-b783-4de9-9658-953deeac34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26256834, 41)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_311.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144e4a3-a7d3-4d6c-9e03-e55333ef1e27",
   "metadata": {},
   "source": [
    "## Part 2 - Columns \n",
    "________________________\n",
    "The rest of the sections in this workbook deal with cleaning, formatting and filtering the dataset to the proper specifications for use in the analysis. The goal is to make sure the complaints are valid complaints and not errors or duplicates by verifying key columns. \n",
    "This section specifically deals with: \n",
    "1. Dropping unwanted columns - since the dataset is so large unneccessary, redundant or empty columns will be dropped first to avoid spending time and computational power on information that will be removed anyway. \n",
    "2. Formatting the column data types - the next step involves formatting the remaining columns into their correct datatypes so that they can be more easily worked with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead85ce-b402-4d79-a9d1-239429a3e979",
   "metadata": {},
   "source": [
    "### Dropping Columns \n",
    "_________________________________________________________________________\n",
    "This section provides a brief description of the columns being removed and the reasoning behind their removal from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473d1e9-d58a-48db-94f1-5a0a975b8fd6",
   "metadata": {},
   "source": [
    "This list displays all of the columns in the dataset alongside the number of null values within each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc46ed1c-4022-433e-8538-bfcc3c9c2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique Key                               0\n",
       "Created Date                             0\n",
       "Closed Date                         726090\n",
       "Agency                                   0\n",
       "Agency Name                              0\n",
       "Complaint Type                           0\n",
       "Descriptor                          395623\n",
       "Location Type                      6292076\n",
       "Incident Zip                       1397334\n",
       "Incident Address                   4389721\n",
       "Street Name                        4391260\n",
       "Cross Street 1                     8287165\n",
       "Cross Street 2                     8406104\n",
       "Intersection Street 1             19490857\n",
       "Intersection Street 2             19497258\n",
       "Address Type                       4052072\n",
       "City                               1630394\n",
       "Landmark                          23178569\n",
       "Facility Type                     20754865\n",
       "Status                                   0\n",
       "Due Date                          17588987\n",
       "Resolution Description              544603\n",
       "Resolution Action Updated Date      399433\n",
       "Community Board                      45340\n",
       "BBL                                6188909\n",
       "Borough                              45340\n",
       "X Coordinate (State Plane)         2141795\n",
       "Y Coordinate (State Plane)         2141664\n",
       "Open Data Channel Type                   0\n",
       "Park Facility Name                     632\n",
       "Park Borough                         45340\n",
       "Vehicle Type                      26247516\n",
       "Taxi Company Borough              26235484\n",
       "Taxi Pick Up Location             26055942\n",
       "Bridge Highway Name               26194339\n",
       "Bridge Highway Direction          26195902\n",
       "Road Ramp                         26197086\n",
       "Bridge Highway Segment            26191184\n",
       "Latitude                           2141807\n",
       "Longitude                          2141807\n",
       "Location                           2141807\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying all of the columns and their missing values\n",
    "nyc_311.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e87db0-dad6-46b2-9e8e-ae7f71ef9cb3",
   "metadata": {},
   "source": [
    "#### Empty Columns \n",
    "These columns have such high numbers of missing values that the columns are rendered useless and will be dropped as a group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2208c48-55a5-4f61-b609-7273b273403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all of these columns which have nothing but null values\n",
    "nyc_top = nyc_311.drop(columns = [\"Vehicle Type\", \n",
    "                                  \"Taxi Company Borough\", \n",
    "                                  \"Taxi Pick Up Location\", \n",
    "                                  \"Bridge Highway Name\", \n",
    "                                  \"Bridge Highway Direction\", \n",
    "                                  \"Road Ramp\", \n",
    "                                  \"Bridge Highway Segment\"\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c80fc2-2923-4d42-aa28-924154e13358",
   "metadata": {},
   "source": [
    "#### Redundant Columns\n",
    "These columns contain information that is found elsewhere within the table and are considered to be redundant. Each will be listed with a brief description of the column and the reason for removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095032b-4c78-4bc6-8ae3-c685f5d3e321",
   "metadata": {},
   "source": [
    "- \"Agency Name\" - this column is already encoded in the \"Agency\" column, if the full name of the agency is needed it can be found easily. Because of this redundancy the column will be dropped.\n",
    "All location data except for the \"Latitude\" and \"Longitude columns will be dropped to reduce conflict between the variables as most of the other data columns are redundant.\n",
    "- \"Landmark\" - this column indicates if there is a notable landmark associated with the location of the service request, it is redundant since latitude and longitude data are available. And because the sheer number of landmarks within the dataset puts this attribute outside the scope of this analysis.\n",
    "- \"X Coordinate (State Plane)\" & \"Y Coordinate (State Plane)\" - these columns show the grid locations of each service request on the state unified plane. This plane is designed to help the government entities within the State of New York better coordinate location information in a single unified way. However for the purposes of this analsis this information is redundant and will be dropped. Source: https://gis.ny.gov/coordinationprogram/workgroups/wg_1/related/standards/datum.htm\n",
    "- \"Intersection Street 1\" & \"Intersection Street 2\" - these columns display the nearest intersections on either side of the service request location, they are used for referencing blocks within each street. This information adds unneccessary complexity for the purposes of this analysis, the location of the requests will be encoded through the \"Latitude\" and \"Longitude\" columns instead.\n",
    "- \"Location\" - this column is just a combination of the \"Latitude\" & \"Longitude\" columns and is redundant, so it will be dropped. \n",
    "- \"Complaint Type\" - the complaint type is encoded by the \"Descriptor\" column and is therefore redundant.\n",
    "- \"Incident Zip\" - indicates the zip code of the complaint. \n",
    "- \"Incident Address\" - indicates the house/building/apartment number referenced in the complaint.\n",
    "- \"Street Name\" - name of the street for the complaint.\n",
    "- \"Cross Street 1\" & \"Cross Street 2\" - these columns show the nearest cross streets to the complaint location.\n",
    "- \"Location Type\" - this column indicates the type of location the service request was made regarding, such as subway, sidewalk, street etc.\n",
    "- \"Address Type\" - references the type of address for the complaint, whether it is a block address or a house/building address.\n",
    "- \"City\" - shows the city within NYC that the complaint is from.\n",
    "- \"Borough\" - shows the borough within NYC that the complaint is from.\n",
    "- \"Park Facility Name\" - name of the park referenced in the complaint, similar to \"Landmark\".\n",
    "- \"Park Borough\" - shows the borough within NYC that the park referenced in the complaint is from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4058520-ab78-4e92-bb4e-5dee96615bb9",
   "metadata": {},
   "source": [
    "Dropping the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0fb88f-c776-461b-b6c2-7d86468e89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the redundant columns\n",
    "nyc_top = nyc_top.drop(columns = [\"Agency Name\",  # Already encoded in the Agency Field, the full names can be found by the acronym\n",
    "                                  \"Landmark\",  # Name of landmark if applicable - Vast number of NaN values and largely unneeded\n",
    "                                  \"X Coordinate (State Plane)\", # NY State Grid Coordinate- Redundant\n",
    "                                  \"Y Coordinate (State Plane)\", # NY State Grid Coordinate- Redundant\n",
    "                                  \"Intersection Street 1\", # Similiar to Cross Street but has more NaN values\n",
    "                                  \"Intersection Street 2\", # Similiar to Cross Street but has more NaN values\n",
    "                                  \"Location\", # Combination of latitude and longitude that is redundant\n",
    "                                  \"Complaint Type\", # The category of the complaint, already encoded in \"Descriptor\"\n",
    "                                  \"Incident Zip\", # Zip code of the complaint, redundant\n",
    "                                  \"Incident Address\", # House/building/apartment number of complaint\n",
    "                                  \"Street Name\", # Name of the street for the complaint\n",
    "                                  \"Cross Street 1\", # Cross street nearest complaint address\n",
    "                                  \"Cross Street 2\", # Second cross street nearest complaint address\n",
    "                                  \"Location Type\", # Type of location referenced in complaint\n",
    "                                  \"Address Type\", # The type of address provided for the complaint\n",
    "                                  \"City\", # City of the complaint address\n",
    "                                  \"Borough\", # Borough of the complaint address\n",
    "                                  \"Park Facility Name\", # Name of the park referenced in the complaint\n",
    "                                  \"Park Borough\" # Borough of the park referenced in the complaint\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7492e04-7214-4335-ba79-ac3c5020f202",
   "metadata": {},
   "source": [
    "#### Unneeded Columns\n",
    "These columns contain information that is either irrelevant or too granular for use in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2b422-28d2-4afc-91cb-102c6ac0c92e",
   "metadata": {},
   "source": [
    "- \"Community Board\" - this is similar to a neighborhood council that has citizen involvement in the administration of government services. This data is excessive for the analysis being performed and the effects will most likely be captured within the location data as the boards are assigned to geographic areas.\n",
    "- \"Facility Type\" - this column references the type of NYC government building that is subject to a complaint, but only if the complaint is about a government service or building. Irrelevant information for this analysis.\n",
    "- \"BBL\" - indicates the parcel id for the building referenced in the complaint, used by certain departments such as city planning to more accurately define and accesss records for complaint buildings.\n",
    "- \"Due Date\" - this column shows the date by which the NYC governmental agency responsible for the complaint should respond to the complaint. For this analysis the due date is irrelevant, although this data may be accessed as part of additional analysis to compare the predictions made by the modeling to the standard expected by the city.\n",
    "- \"Resolution Description\" - this column gives the action taken by the responding agency to rectify the complaint. Although this column could be useful for determining the length and quality of response it is post hoc information. Meaning that it is given after the event has occured and cannot be used as a predictor variable.\n",
    "- \"Resolution Action Updated Date\" - this column gives the timestamp of the last update to the resolution given by the responsible agency. This column is unnecessary as the metric of concern is the time to completion for each complaint not the last update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ee6d0-b595-4ae4-93ed-ec2c8466bd0e",
   "metadata": {},
   "source": [
    "Dropping the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bbdde2-bec8-4b70-bbf6-fc2086b55712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unneeded columns\n",
    "nyc_top = nyc_top.drop(columns = [\"Community Board\", # Citizen Board Jurisdiction\n",
    "                                  \"Facility Type\", # Type of NYC government building referenced in complaint\n",
    "                                  \"BBL\", # Land Parcel ID\n",
    "                                  \"Due Date\", # Date by which the department responsible for the complaint should respond to the complaint\n",
    "                                  \"Resolution Description\", # Description of action taken by the responding agency\n",
    "                                  \"Resolution Action Updated Date\" # When the resolution action was last updated \n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c0cec-40a4-4efb-9b47-dc902a68ce09",
   "metadata": {},
   "source": [
    "The old index column for the complaints was carried over and is now irrelevant so it will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3df9e-9c95-4607-be17-e80d09fdc0de",
   "metadata": {},
   "source": [
    "### Top Ten Descriptors \n",
    "_____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973f563-7048-4c03-a783-c4b12c59eadf",
   "metadata": {},
   "source": [
    "The \"Descriptor\" column in this data set contains descriptions of complaints submitted to the 311 system. These descriptions are more granular than complaint types and are better categorization method for complaints than \"Complaint Type\"s which are too broad to be useful. More analysis will be performed on these top ten descriptors, to facilitate this the descriptors will be updated to more informative names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc58eea-78e9-4881-afe8-03be56fdd84b",
   "metadata": {},
   "source": [
    "#### Getting the top ten descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b80dc7-3067-4012-ae63-238f024d820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of complaints by descriptor\n",
    "com_descriptor_num = nyc_top[\"Descriptor\"].value_counts()\n",
    "# Sorting by number of complaints\n",
    "com_descriptor_num_sorted = com_descriptor_num.sort_values(ascending = False)\n",
    "# Getting the top ten descriptors\n",
    "top_ten_descriptors = com_descriptor_num_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b43cdb-c424-4e2b-b193-171ba9eb91fc",
   "metadata": {},
   "source": [
    "Creating read-outs for each of the top ten descriptors with the number of complaints and the break down of descriptors by complaint type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89108b3c-74b6-43b1-b7a8-90a284db6e20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The complaints with the Loud Music/Party descriptor fall under these complaint types:\n",
      "Noise - Residential         1631891\n",
      "Noise - Street/Sidewalk      604129\n",
      "Noise - Commercial           317268\n",
      "Noise - Park                  38792\n",
      "Noise - House of Worship       7729\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the Loud Music/Party descriptor is 2599809\n",
      "\n",
      "\n",
      "The complaints with the ENTIRE BUILDING descriptor fall under these complaint types:\n",
      "HEAT/HOT WATER    981577\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the ENTIRE BUILDING descriptor is 981577\n",
      "\n",
      "\n",
      "The complaints with the HEAT descriptor fall under these complaint types:\n",
      "HEATING    868960\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the HEAT descriptor is 868960\n",
      "\n",
      "\n",
      "The complaints with the No Access descriptor fall under these complaint types:\n",
      "Blocked Driveway    837875\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the No Access descriptor is 837875\n",
      "\n",
      "\n",
      "The complaints with the Street Light Out descriptor fall under these complaint types:\n",
      "Street Light Condition    751210\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the Street Light Out descriptor is 751210\n",
      "\n",
      "\n",
      "The complaints with the Banging/Pounding descriptor fall under these complaint types:\n",
      "Noise - Residential         615778\n",
      "Noise - Commercial           44380\n",
      "Noise - House of Worship      1300\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the Banging/Pounding descriptor is 661458\n",
      "\n",
      "\n",
      "The complaints with the Pothole descriptor fall under these complaint types:\n",
      "Street Condition    644323\n",
      "Bridge Condition      2054\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the Pothole descriptor is 646377\n",
      "\n",
      "\n",
      "The complaints with the APARTMENT ONLY descriptor fall under these complaint types:\n",
      "HEAT/HOT WATER    525424\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the APARTMENT ONLY descriptor is 525424\n",
      "\n",
      "\n",
      "The complaints with the Loud Talking descriptor fall under these complaint types:\n",
      "Noise - Street/Sidewalk     190082\n",
      "Noise - Residential         146596\n",
      "Noise - Commercial           38050\n",
      "Noise - Park                  9771\n",
      "Noise - House of Worship      2337\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the Loud Talking descriptor is 386836\n",
      "\n",
      "\n",
      "The complaints with the CEILING descriptor fall under these complaint types:\n",
      "PAINT/PLASTER      188821\n",
      "PAINT - PLASTER    180010\n",
      "Name: Complaint Type, dtype: int64\n",
      "The total number of complaints with the CEILING descriptor is 368831\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing the empty descriptor list\n",
    "descriptor_list = []\n",
    "\n",
    "# Initiating a for llop over the top ten descriptors for compalints\n",
    "for counter, descriptor in enumerate(top_ten_descriptors):\n",
    "    \n",
    "    # Assigning the index of the descriptor to a variable\n",
    "    index = top_ten_descriptors.index[counter]\n",
    "    \n",
    "    # Assigning the index of the complaint type to a variable\n",
    "    value = top_ten_descriptors[counter]\n",
    "    \n",
    "    # Creates a readout of the complaint types for the descriptor in question\n",
    "    print(f\"The complaints with the {index} descriptor fall under these complaint types:\")\n",
    "    complaint_type = nyc_311[\"Complaint Type\"].loc[nyc_311[\"Descriptor\"] == index].value_counts()\n",
    "    print(complaint_type)\n",
    "    \n",
    "    # Creates a readout showing the descriptor and the total number of complaints with that descriptor\n",
    "    descriptor_var = nyc_311.loc[nyc_311[\"Descriptor\"] == index]\n",
    "    print(f\"The total number of complaints with the {index} descriptor is {len(descriptor_var)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Appends the descriptor to the descriptor list\n",
    "    descriptor_list.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bf620-f7a4-4ba2-ac85-3de0f4f468fa",
   "metadata": {},
   "source": [
    "The descriptors above will be the primary method of categorizing complaints used in this analysis, for the sake of readability the descriptor columns will be amended to give more context to the descriptor. For example \"ENTIRE BUILDING\" is too vauge to be useful, it will become \"ENTIRE BUILDING - HEAT/HOT WATER OUT\" which better conveys the needed information. This must be done manually as there are multiple complaint types for several descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805120a-059c-4b69-8b48-a8ce8152046f",
   "metadata": {},
   "source": [
    "#### 1. \"Loud Music/Party\" -> \"LOUD MUSIC/PARTY - NOISE\"\n",
    "This descriptor is used for any loud music/party sounds, its classified as a general noise complaint that has a complaint type based on where the party/loud music is reported.\n",
    "However it will just be labeled as \"- NOISE\" for simplicity as that conveys enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a55be8-5896-4ffe-9d54-0c08b9464543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"Loud Music/Party\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"LOUD MUSIC/PARTY - NOISE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646aff37-06a0-4aa2-81cb-3680587b7afd",
   "metadata": {},
   "source": [
    "#### 2. \"ENTIRE BUILDING\" -> \"ENTIRE BUILDING - HEAT/HOT WATER OUT\"\n",
    "This descriptor indicates that the most or all of a buildings heating or hot water has gone out. There are several other heating related descriptors in the top ten that deal with outages that are either unknown or specific to a single apartment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b452ac-105c-4c63-9197-da8ed809a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"ENTIRE BUILDING\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"ENTIRE BUILDING - HEAT/HOT WATER OUT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34330942-4420-4c8c-8e92-6add93f9c51f",
   "metadata": {},
   "source": [
    "#### 3. \"HEAT\" -> \"HEAT OUT - GENERAL\"\n",
    "This descriptor indicates a heat outage of an unknown size. It is given the designation \"- GENERAL\" as not much more is known about the scale of outages for this descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "466f3842-c448-4a6e-9fa9-4429290dab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"HEAT\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"HEAT OUT - GENERAL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b9299-ab2f-40aa-8413-771694213007",
   "metadata": {},
   "source": [
    "#### 4. \"No Access\" -> \"NO ACCESS - BLOCKED DRIVEWAY\"\n",
    "This descriptor indicates a blocked driveway complaint. Because the only complaint category for this descriptor is for a blocked driveway it has been labeled \"- BLOCKED DRIVEWAY\" to give enough context to interpret the descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a11f005-4d6b-468f-a202-7dc411119c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"No Access\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"NO ACCESS - BLOCKED DRIVEWAY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4177242-c84c-48ea-98a9-1fc81267bb9a",
   "metadata": {},
   "source": [
    "#### 5. \"Street Light Out\" -> \"STREET LIGHT OUT\"\n",
    "This descriptor is self explantory and will only be changed into all capital letters for the sake of consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "179a699d-c7ce-452c-9333-38826189cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"Street Light Out\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"STREET LIGHT OUT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410bacc-cfd1-412d-9d6f-05673deb8785",
   "metadata": {},
   "source": [
    "#### 6. \"Banging/Pounding\" -> \"BANGING/POUNDING - NOISE\"\n",
    "This descriptor details a specific banging/pounding noise complaint, \"- NOISE\" will be added for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63990b05-a77f-4a53-866c-8b73d95378a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"Banging/Pounding\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"BANGING/POUNDING - NOISE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b5104-0d42-4611-b23e-d85ed067f330",
   "metadata": {},
   "source": [
    "#### 7. \"Pothole\" -> \"POTHOLE\"\n",
    "The descriptor is fairly self explanatory, the descriptor will be formatted in all capital letters for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f46c370-fdd4-4029-b6bb-6b22721acb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"Pothole\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"POTHOLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a29e9-193e-4a94-9892-768505098593",
   "metadata": {},
   "source": [
    "#### 8. \"APARTMENT ONLY\" -> \"APARTMENT ONLY - HEAT/HOT WATER OUT\"\n",
    "This descriptor is a continuation of the \"Heat/Hot Water Out\" complaint type and indicates an apartment specific outage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef297662-0021-46fa-bfb3-6127efba4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"APARTMENT ONLY\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"APARTMENT ONLY - HEAT/HOT WATER OUT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c76905-10d6-4a0d-9fc8-35ffa495252a",
   "metadata": {},
   "source": [
    "#### 9. \"Loud Talking\" -> \"LOUD TALKING - NOISE\"\n",
    "This descriptor is a continuation of the \"- Noise\" complaint type and will be marked with \" - NOISE\" to indicate a noise complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e87e36df-5851-42b5-b3db-e4b7cde829d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"Loud Talking\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"LOUD TALKING - NOISE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2ed2e-bed0-4d90-b347-09e17d5d0612",
   "metadata": {},
   "source": [
    "#### 10. \"CEILING\" -> \"CEILING - PAINT/PLASTER REPAIR\"\n",
    "This descriptor indicates a complaint filed with the city alledging that a repair to fix a ceiling with paint or plaster is not being performed by the owners or co-op board of an apartment building. \n",
    "By law these repairs must be made to ensure livability, the descriptor will be annotated with \"- PAINT/PLASTER REPAIR\" to give context.\n",
    "Source: https://portal.311.nyc.gov/article/?kanumber=KA-01074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a258cab6-d58b-4d10-b6cc-6ddbfcabf35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable for the descriptor mask\n",
    "describe = nyc_top[\"Descriptor\"] == \"CEILING\"\n",
    "\n",
    "#Changing all of the values for the descriptor to the annotated version\n",
    "nyc_top.loc[describe, \"Descriptor\"] = \"CEILING - PAINT/PLASTER REPAIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e9c09-fe7a-4839-9fba-69cedb58704f",
   "metadata": {},
   "source": [
    "#### Checking that the descriptor renaming worked by recompiling the top ten descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7c853c-19c2-42c5-a95e-78ebd3ad3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOUD MUSIC/PARTY - NOISE                2599809\n",
       "ENTIRE BUILDING - HEAT/HOT WATER OUT     981577\n",
       "HEAT OUT - GENERAL                       868960\n",
       "NO ACCESS - BLOCKED DRIVEWAY             837875\n",
       "STREET LIGHT OUT                         751210\n",
       "BANGING/POUNDING - NOISE                 661458\n",
       "POTHOLE                                  646377\n",
       "APARTMENT ONLY - HEAT/HOT WATER OUT      525424\n",
       "LOUD TALKING - NOISE                     386836\n",
       "CEILING - PAINT/PLASTER REPAIR           368831\n",
       "Name: Descriptor, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the number of complaints by descriptor\n",
    "com_descriptor_num = nyc_top[\"Descriptor\"].value_counts()\n",
    "# Sorting by number of complaints\n",
    "com_descriptor_num_sorted = com_descriptor_num.sort_values(ascending = False)\n",
    "# Getting the top ten descriptors\n",
    "top_ten_descriptors = com_descriptor_num_sorted[:10]\n",
    "# Displaying the top ten descriptors\n",
    "top_ten_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e5f7661-ca25-4c28-9ac0-652b8f5447ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of complaints for the top ten descriptors is 8628357 out of 26256834 total complaints.\n"
     ]
    }
   ],
   "source": [
    "# Summing the total number of compaints\n",
    "top_ten_sum = top_ten_descriptors.sum()\n",
    "print(f\"The total number of complaints for the top ten descriptors is {top_ten_sum} out of {len(nyc_top)} total complaints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74794e-cc12-4c88-87cc-62532d2cd10a",
   "metadata": {},
   "source": [
    "### Formatting the column data types\n",
    "_______________________________________________\n",
    "One of the columns within the dataset has a sub-optimal datatype for use in the cleaning process. The change in datatype and the reasoning behind it is explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "322418bc-2a3a-4906-9303-2a54d162b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26256834 entries, 0 to 26256833\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   Unique Key              int64  \n",
      " 1   Created Date            object \n",
      " 2   Closed Date             object \n",
      " 3   Agency                  object \n",
      " 4   Descriptor              object \n",
      " 5   Status                  object \n",
      " 6   Open Data Channel Type  object \n",
      " 7   Latitude                float64\n",
      " 8   Longitude               float64\n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# Looking at the datatypes for each of the columns\n",
    "nyc_top.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c5b2f-fdbc-4c16-986b-b8437738717a",
   "metadata": {},
   "source": [
    "Most of the columns are in acceptable formats, the vast majority are classed as objects because they contain strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95300d-d4dc-4520-9f19-819759c15a26",
   "metadata": {},
   "source": [
    "The \"Created Date\" column needs to be converted into 'datetime64[ns]' format, this will allow for more advanced functions to be performed on the column and making working with the time data easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f907b793-6439-47d2-9c9f-c5c091bb31b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:31:09.053874\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Converting the \"Created Date\"\n",
    "nyc_top[\"Created Date\"] = pd.to_datetime(nyc_top[\"Created Date\"])\n",
    "finish = datetime.now()\n",
    "duration = finish - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e3a7bf7-989d-4b07-ba87-8b64dc3d4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26256834 entries, 0 to 26256833\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   Unique Key              int64         \n",
      " 1   Created Date            datetime64[ns]\n",
      " 2   Closed Date             object        \n",
      " 3   Agency                  object        \n",
      " 4   Descriptor              object        \n",
      " 5   Status                  object        \n",
      " 6   Open Data Channel Type  object        \n",
      " 7   Latitude                float64       \n",
      " 8   Longitude               float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(5)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# Checking for successful conversion\n",
    "nyc_top.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc1db8-d201-46f9-bb0f-54826758e929",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the data types have been successfuly changed a backup copy of the data will be created. This is just for the convenience of the viewer so that the computationally expensive process of re-formatting the columns isn't necessary if the data needs to be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "298c5408-69ae-4afa-ae2f-cccd2b9e8a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26256834 entries, 0 to 26256833\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   Unique Key              int64         \n",
      " 1   Created Date            datetime64[ns]\n",
      " 2   Closed Date             object        \n",
      " 3   Agency                  object        \n",
      " 4   Descriptor              object        \n",
      " 5   Status                  object        \n",
      " 6   Open Data Channel Type  object        \n",
      " 7   Latitude                float64       \n",
      " 8   Longitude               float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(5)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# Creating a backup copy to avoid reformatting columns if an error occurs\n",
    "nyc_top_backup = nyc_top\n",
    "nyc_top_backup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eead033-fb6b-4998-b28c-210635415d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to reset nyc_top to backup state\n",
    "nyc_top = nyc_top_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c0c27-b046-4c4a-b7b8-be8d8642c1ca",
   "metadata": {},
   "source": [
    "## Part 3 - Rows \n",
    "___________________________\n",
    "The next step in cleaning this dataset is to address inconsistencies within the data. Primarily null or duplicate values. Context for each set of missing data and the actions taken to address the issue are provided below in addition to the reasoning for each decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6b6dd-37d9-4454-957e-5db97394f90a",
   "metadata": {},
   "source": [
    "### Dealing with null values\n",
    "__________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dce2d01c-baf3-4425-8f1d-b54fcaa2b529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique Key                      0\n",
       "Created Date                    0\n",
       "Closed Date                726090\n",
       "Agency                          0\n",
       "Descriptor                 395623\n",
       "Status                          0\n",
       "Open Data Channel Type          0\n",
       "Latitude                  2141807\n",
       "Longitude                 2141807\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying all of the null value counts for the dataframe\n",
    "nyc_top.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98226585-4fd9-4397-b6fd-13a30921055d",
   "metadata": {},
   "source": [
    "#### Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0859abed-0331-41fc-87b7-0653d75ae8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with NaN latitude values is 2141807.\n",
      "The number of rows with NaN longitude values is 2141807.\n",
      "The total number of rows in the dataset is 26256834.\n",
      "The rows with missing location values account for 8.16% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "rows = len(nyc_top)\n",
    "\n",
    "# Creating masks for null values\n",
    "lat_na = len(nyc_top[nyc_top[\"Latitude\"].isna() == True])\n",
    "lon_na = len(nyc_top[nyc_top[\"Longitude\"].isna() == True])\n",
    "\n",
    "# Printing the number of rows with null values for the column\n",
    "print(f\"The number of rows with NaN latitude values is {lat_na}.\")\n",
    "print(f\"The number of rows with NaN longitude values is {lon_na}.\")\n",
    "\n",
    "# Displaying the number of rows in the dataset\n",
    "print(f\"The total number of rows in the dataset is {rows}.\")\n",
    "\n",
    "# Displaying the percentage of rows in the dataset with missing values\n",
    "print(f\"The rows with missing location values account for {round((lat_na/rows)*100,2)}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4a987-088d-4db8-8219-d0af1dcd49d7",
   "metadata": {},
   "source": [
    "All of the rows that have missing \"Latitude\" values also have missing \"Longitude\" values, this suggests that the geolocation data usually obtained by the responding agency was corrupted, lost or never collected. Resulting in an address and associated information being provided without the geolocation data verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f038dd7-ac02-49c2-9e3a-5d44c2b4b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five addresses with the most missing location data: \n",
      " BELT PKWY                5713\n",
      "BKLYN QUEENS EXPY        5001\n",
      "CENTRAL PARK             4711\n",
      "GRAND CENTRAL PKWY       4557\n",
      "FLUSHING MEADOWS PARK    4439\n",
      "Name: Incident Address, dtype: int64\n",
      "\n",
      "\n",
      "The five addresses with the least missing location data: \n",
      " 96 86 STREET           1\n",
      "90-29 56 AVENUE        1\n",
      "155 NEW DORP LANE      1\n",
      "1325 EAST 29 STREET    1\n",
      "145105 DIVISION AVE    1\n",
      "Name: Incident Address, dtype: int64\n",
      "\n",
      "\n",
      "There are 165763 unique addresses with missing Latitude and Longitude data.\n",
      "The address with the most missing values accounts for just 0.27% of the missing Latitude and Longitude data.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: nyc_311 was used to check the number of missing values per address \n",
    "\n",
    "# Calculating the number of missing \"Latitude\" values for each unique address within the data\n",
    "counts_addresses = nyc_311[\"Incident Address\"][nyc_311[\"Latitude\"].isna()].value_counts() \n",
    "\n",
    "# Calculating the number of unique addresses in the dataset\n",
    "unique_addresses = len(nyc_311[\"Incident Address\"][nyc_311[\"Latitude\"].isna()].unique())\n",
    "\n",
    "# Calculating the percentage of missing \"Latitude\" and \"Longitude\" data for the address with the most missing data\n",
    "most_missing = round((counts_addresses[0]/lat_na)*100, 2)\n",
    "\n",
    "# Printing various statistics \n",
    "print(\"The five addresses with the most missing location data: \\n\", counts_addresses.head(5))\n",
    "print(\"\\n\")\n",
    "print(\"The five addresses with the least missing location data: \\n\", counts_addresses.tail(5))\n",
    "print(\"\\n\")\n",
    "print(f\"There are {unique_addresses} unique addresses with missing Latitude and Longitude data.\")\n",
    "print(f\"The address with the most missing values accounts for just {most_missing}% of the missing Latitude and Longitude data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f3fc7-c3ec-4f12-9e2b-dfb9b23464be",
   "metadata": {},
   "source": [
    "Due to the scope of missing data as illustrated above the rows with missing \"Latitude\" and \"Longitude\" data will be dropped. It is impractical due to the time constraints on this analysis to attempt to rectify such a large number of missing values in such small increments. As noted above the rows with missing values account for only 4.58% of the dataset, and since the entire dataset is 8.62 million rows long the loss of under 400 thousands rows should not effect the viability of the analysis or the resulting models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4705bd1-ed5a-4729-ae25-66c500600704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing latitude values: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtering out all of the rows with missing \"Latitude\" data\n",
    "nyc_top = nyc_top[nyc_top[\"Latitude\"].isna() == False]\n",
    "\n",
    "# Checking how many rows have missing \"Latitude\" values\n",
    "missing_lat = len(nyc_top[nyc_top[\"Latitude\"].isna() == True])\n",
    "\n",
    "# Checking if there are any rows with undesired values\n",
    "print(f\"Rows with missing latitude values: {missing_lat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ae990",
   "metadata": {},
   "source": [
    "The filtering of missing \"Latitude\" data was successful, as expected all of the rows with missing longitude data were filtered out as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954276e0-53d4-473e-9c02-b3bd45b92604",
   "metadata": {},
   "source": [
    "#### Closed Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cdbfb0a-4705-4434-8cd4-1d79f869c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique Key                     0\n",
       "Created Date                   0\n",
       "Closed Date               684826\n",
       "Agency                         0\n",
       "Descriptor                320894\n",
       "Status                         0\n",
       "Open Data Channel Type         0\n",
       "Latitude                       0\n",
       "Longitude                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying all of the null value counts for the dataframe\n",
    "nyc_top.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1648519-5e9b-44c2-abde-deb90aeac416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with missing Closed Dates is 684826.\n",
      "The rows that have missing Closed Dates for 2.61% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "len(nyc_top)\n",
    "\n",
    "# Checking how many rows have missing \"Closed Date\" values\n",
    "closed_na = len(nyc_top[nyc_top[\"Closed Date\"].isna() == True])\n",
    "print(f\"The number of rows with missing Closed Dates is {closed_na}.\")\n",
    "\n",
    "# Calculating and displaying the percentage of rows being dropped\n",
    "print(f\"The rows that have missing Closed Dates for {round((closed_na/rows)*100,2)}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a6513-feb0-436f-b81f-574f74d49f78",
   "metadata": {},
   "source": [
    "The \"Closed Date\" column shows the day on which the complaint was considered closed by the agency responsible for the complaint. For this analysis only those complaints that are closed will be considered. The majority of the missing values come from \"Open\" complaints but some arise from errors or misclassifications. The rows with missing \"Closed Date\"s will be dropped. Additional filtering will take place in a subsequent step to filter out all statuses other than closed complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f7004a5-4b9e-4027-80f6-7b144d1293bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing Closed Dates: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtering out all of the rows with missing \"Closed Date\" data\n",
    "nyc_top = nyc_top[nyc_top[\"Closed Date\"].isna() == False]\n",
    "\n",
    "# Checking how many rows have missing \"Latitude\" values\n",
    "missing_closed = len(nyc_top[nyc_top[\"Closed Date\"].isna() == True])\n",
    "\n",
    "# Checking if there are any rows with undesired values\n",
    "print(f\"Rows with missing Closed Dates: {missing_closed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466770c-d383-4c93-b7e8-0edd37e5e8d3",
   "metadata": {},
   "source": [
    "The drop was succesful. Only one more set of missing data values remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e6c9795-b5d6-4612-a686-0109bf6b6edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique Key                     0\n",
       "Created Date                   0\n",
       "Closed Date                    0\n",
       "Agency                         0\n",
       "Descriptor                314438\n",
       "Status                         0\n",
       "Open Data Channel Type         0\n",
       "Latitude                       0\n",
       "Longitude                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_top.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d02b579d-fe37-40e3-850a-0fa8caaded97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with missing descriptors is 314438.\n",
      "The rows that have missing descriptors account for 1.2% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "len(nyc_top)\n",
    "\n",
    "# Checking how many rows have missing \"Descriptor\" values\n",
    "descriptor_na = len(nyc_top[nyc_top[\"Descriptor\"].isna() == True])\n",
    "print(f\"The number of rows with missing descriptors is {descriptor_na}.\")\n",
    "\n",
    "# Calculating and displaying the percentage of rows being dropped\n",
    "print(f\"The rows that have missing descriptors account for {round((descriptor_na/rows)*100,2)}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2328caf-afb3-464f-885b-a4e92daf2380",
   "metadata": {},
   "source": [
    "The \"Descriptor\" columns give the subset of complaint type for the SR request. Only those SRs that have a valid \"Descriptor\" value will be considered to ensure only valid SRs are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e402eb9f-6f75-4946-a503-0ddc317cf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing Closed Dates: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtering out all of the rows with missing \"Descriptor\" data\n",
    "nyc_top = nyc_top[nyc_top[\"Descriptor\"].isna() == False]\n",
    "\n",
    "# Checking how many rows have missing \"Latitude\" values\n",
    "missing_descriptor = len(nyc_top[nyc_top[\"Descriptor\"].isna() == True])\n",
    "\n",
    "# Checking if there are any rows with undesired values\n",
    "print(f\"Rows with missing Closed Dates: {missing_descriptor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed11791-677a-45ab-92fb-72e139134b9b",
   "metadata": {},
   "source": [
    "The drop was successful. The next step is searching for duplicates within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a2b6c-cdcd-4868-b76f-a2a9715267d7",
   "metadata": {},
   "source": [
    "### Checking for duplicates\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90d32638-2e24-4c5c-a5a6-d320eb56cdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    23115763\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_top.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde9766-9f04-4834-9240-99af741476d7",
   "metadata": {},
   "source": [
    "There don't appear to be any duplicate values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ebd06-fc3a-4d7c-904f-7a30c0aaeac6",
   "metadata": {},
   "source": [
    "### Resetting the Index\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77673985-cd9e-496f-bffb-31499d521ec2",
   "metadata": {},
   "source": [
    "Now that all of the drops have been performed on the datasets the index will be reset to make the index values contiguous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2eb709a2-2935-4e00-bdf7-07b1a37922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index\n",
    "nyc_top.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0a9c4d3-923d-46c2-abb5-45b64a3a23d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique Key</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Closed Date</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Descriptor</th>\n",
       "      <th>Status</th>\n",
       "      <th>Open Data Channel Type</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48330841</td>\n",
       "      <td>2020-12-05 22:00:30</td>\n",
       "      <td>12/05/2020 10:14:35 PM</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>LOUD MUSIC/PARTY - NOISE</td>\n",
       "      <td>Closed</td>\n",
       "      <td>MOBILE</td>\n",
       "      <td>40.841317</td>\n",
       "      <td>-73.936608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48330856</td>\n",
       "      <td>2020-12-05 21:19:55</td>\n",
       "      <td>12/05/2020 10:49:08 PM</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>LOUD MUSIC/PARTY - NOISE</td>\n",
       "      <td>Closed</td>\n",
       "      <td>MOBILE</td>\n",
       "      <td>40.833438</td>\n",
       "      <td>-73.945336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48330864</td>\n",
       "      <td>2020-12-06 00:38:54</td>\n",
       "      <td>12/06/2020 01:06:35 AM</td>\n",
       "      <td>NYPD</td>\n",
       "      <td>LOUD MUSIC/PARTY - NOISE</td>\n",
       "      <td>Closed</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>40.707605</td>\n",
       "      <td>-74.005720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique Key        Created Date             Closed Date Agency  \\\n",
       "0    48330841 2020-12-05 22:00:30  12/05/2020 10:14:35 PM   NYPD   \n",
       "1    48330856 2020-12-05 21:19:55  12/05/2020 10:49:08 PM   NYPD   \n",
       "2    48330864 2020-12-06 00:38:54  12/06/2020 01:06:35 AM   NYPD   \n",
       "\n",
       "                 Descriptor  Status Open Data Channel Type   Latitude  \\\n",
       "0  LOUD MUSIC/PARTY - NOISE  Closed                 MOBILE  40.841317   \n",
       "1  LOUD MUSIC/PARTY - NOISE  Closed                 MOBILE  40.833438   \n",
       "2  LOUD MUSIC/PARTY - NOISE  Closed                 ONLINE  40.707605   \n",
       "\n",
       "   Longitude  \n",
       "0 -73.936608  \n",
       "1 -73.945336  \n",
       "2 -74.005720  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for successful completion\n",
    "nyc_top.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbff55c-5fc9-4467-805b-d22ac46cb4e1",
   "metadata": {},
   "source": [
    "The index column was successfully reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28886adb-acb9-442c-b712-f414a4feb715",
   "metadata": {},
   "source": [
    "## Part 4 - Limiting Scope\n",
    "___________________________________________________________________\n",
    "This section deals with limiting the dataset to only those complaints that have both a \"Closed\" status and those complaints filed in 2010-2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5ae01-541b-494a-8e13-016304bd2bd6",
   "metadata": {},
   "source": [
    "### Status\n",
    "________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5513c59b-d783-437a-aa7c-41a2ac04628f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Closed              22949858\n",
       "Pending               130886\n",
       "Assigned               17239\n",
       "Open                   17117\n",
       "In Progress              502\n",
       "Started                  147\n",
       "Closed - Testing           9\n",
       "Unassigned                 5\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of values for each possible complaint \"Status\"\n",
    "nyc_top[\"Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7430975d-1555-4956-ac2d-ba5c0d3832c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with statuses other than Closed is 165905.\n",
      "The rows that have statuses other than Closed account for 0.72% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in the dataset\n",
    "rows = len(nyc_top)\n",
    "\n",
    "# Number of rows with statuses other than \"Closed\"\n",
    "status = len(nyc_top[nyc_top[\"Status\"] != \"Closed\"] )\n",
    "\n",
    "# Displaying the number of rows with statuses other than \"Closed\"\n",
    "print(f\"The number of rows with statuses other than Closed is {status}.\")\n",
    "\n",
    "# Calculating and displaying the percentage of rows being dropped\n",
    "print(f\"The rows that have statuses other than Closed account for {round((status/rows)*100,2)}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a56d5-589a-4a77-9619-bf977f1592d0",
   "metadata": {},
   "source": [
    "The \"Status\" column describes the stage of the complaint handling process each individual complaint is at. A complaint marked \"Closed\" has been responded to and any appropriate actions have been completed by the responsible agencies. For the purposes of this analysis only those complaints that are marked as \"Closed\" will be considered and all other complaints will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c973272-8f3e-4e11-a06b-684408e117d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with unclosed statuses: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtering out all of the rows with missing \"Latitude\" data\n",
    "nyc_top = nyc_top[(nyc_top[\"Status\"] == \"Closed\")]\n",
    "\n",
    "# Checking how many rows have missing \"Latitude\" values\n",
    "unclosed = len(nyc_top[nyc_top[\"Status\"] != \"Closed\"])\n",
    "\n",
    "# Checking if there are any rows with undesired values\n",
    "print(f\"Rows with unclosed statuses: {unclosed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9036938-e301-4b72-91f8-a1e72252e287",
   "metadata": {},
   "source": [
    "The filtering for unclosed complaints was successful. Now that only closed complaints are present within the dataframe the \"Status\" column will be dropped as it is no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1c9dad3-08ad-41bf-a24e-2709c6092448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the \"Status\" column\n",
    "nyc_top.drop(labels = \"Status\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f60032-3af2-418e-b252-353553a34a71",
   "metadata": {},
   "source": [
    "### Timeframe\n",
    "______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31128077-d526-478f-ae2c-d8b5c3731527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created Date\n",
       "2010    1722919\n",
       "2011    1569560\n",
       "2012    1543714\n",
       "2013    1577237\n",
       "2014    1865478\n",
       "2015    2021295\n",
       "2016    2090550\n",
       "2017    2211769\n",
       "2018    2474355\n",
       "2019    2160656\n",
       "2020    2333861\n",
       "2021    1378464\n",
       "Name: Unique Key, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the number of complaints by year\n",
    "nyc_top[\"Unique Key\"].groupby(nyc_top[\"Created Date\"].dt.year).agg(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff038847-b257-40dc-bf39-e5832b8f5931",
   "metadata": {},
   "source": [
    "The original dataset being used for this analysis consisted of 311 complaints from NYC that were collected from 2010 until the time of this analysis in September 2021. Such a large timeframe makes for an unwieldy amount of data for the resources available to this project. In addition since the start of the COVID-19 pandemic in March of 2020 the collection capabilty and quality of data in 311 complaints has suffered drastically. This is due to city services being overwhelmed at in almost every possible way. Leading to a backlog of complaints and failures in data collection. To counter the issues of both size and data quality this analysis will focus only on complaints CLOSED from 2010 to December 2019 and the data will be further reduced in to timeseries format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6868ddaf-a8e6-48c1-b571-103373bc055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complaints from 2020 and 2021: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtering out complaints from 2020 and 2021\n",
    "nyc_top = nyc_top[nyc_top[\"Created Date\"].dt.year <= 2019]\n",
    "\n",
    "# Checking how many rows are from 2020 and 2021\n",
    "years = len(nyc_top[nyc_top[\"Created Date\"].dt.year >= 2020])\n",
    "\n",
    "# Checking if there are any rows with undesired values\n",
    "print(f\"Number of complaints from 2020 and 2021: {years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f04fda-2edc-4e1f-82c2-17cd0d7ee2ea",
   "metadata": {},
   "source": [
    "### Converting the complain dataframe to a .csv file\n",
    "_____________________________\n",
    "The filtering was successful, the data set is now ready for conversion to timeseries format. But the dataframe will first be converted into a .csv for use in EDA in its current form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ec165cc-bb23-4ac3-b67c-8c03d966b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19237533, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dataframe shape\n",
    "nyc_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c90683d9-8e66-4e87-9db0-34255fe419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_top.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f389010-727f-4b4f-92f2-aee8680e0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19237533 entries, 0 to 19237532\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   Unique Key              int64         \n",
      " 1   Created Date            datetime64[ns]\n",
      " 2   Closed Date             object        \n",
      " 3   Agency                  object        \n",
      " 4   Descriptor              object        \n",
      " 5   Open Data Channel Type  object        \n",
      " 6   Latitude                float64       \n",
      " 7   Longitude               float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "# Displaying final information\n",
    "nyc_top.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4348c382-d08d-4f7f-80d5-620e917e7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:27.978603\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Converting the dataframe into a .csv file\n",
    "nyc_top.to_csv('311 Data/nyc_clean_complaint_311.csv')\n",
    "finish = datetime.now()\n",
    "duration = finish - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516f28f-bced-44a8-9d67-271e98752140",
   "metadata": {},
   "source": [
    "## Part 5 - Creating the timeseries dataframe\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45830099-9d6e-4522-a709-cdb4715c67e4",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a85bf7-22c5-4233-b6b4-3ed5132a3df5",
   "metadata": {},
   "source": [
    "The process of taking the cleaned dataset and converting it into a timeseries format is quite simple. Timeseries data tracks only time as an independent variable (X), with the same dependent variables shown through time (y). In this use case there will be only the time index by day and the total number of SRs submitted that day. An aggregation function is used to group all of the SRs together by date and calculate the per day values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c39ec64-b42b-4e03-9fbd-d8672506794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the SRs by day and counting the per day values\n",
    "totals = nyc_top[\"Unique Key\"].groupby(nyc_top[\"Created Date\"].dt.date).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac329859-9cd0-48ce-bdc9-7c1510831b4a",
   "metadata": {},
   "source": [
    "Now that the values have been calculated a dataframe is made and the columns are renamed to reflect their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f79af7df-b5bb-4258-9ee8-11c2f8a6e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the totals\n",
    "time_df = pd.DataFrame(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab3593f5-45d8-45dc-a6dd-a3e10db1eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "time_df.rename(columns = {\"Unique Key\": \"Total SRs\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c17cb1-f4ed-4141-93a3-e3c689ec5db1",
   "metadata": {},
   "source": [
    "Now we can see the first few rows and information on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5389033-95eb-4755-9ef0-3f03964852d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3652 entries, 2010-01-01 to 2019-12-31\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   Total SRs  3652 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 57.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Displaying basic info on the dataset\n",
    "time_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f04a286-0b17-4ed7-b6e6-9e867d97833d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total SRs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Created Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>2782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>3692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>7977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>7119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total SRs\n",
       "Created Date           \n",
       "2010-01-01         2782\n",
       "2010-01-02         3692\n",
       "2010-01-03         5426\n",
       "2010-01-04         7977\n",
       "2010-01-05         7119"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first five rows of the dataset\n",
    "time_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5cac9-7853-40b5-b47d-0f0651fb0d43",
   "metadata": {},
   "source": [
    "### Cleaning the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ade100-2e3e-44c9-8b67-3543a975a925",
   "metadata": {},
   "source": [
    "Now that the data has been successfully gathered and formatted it can be checked for inconsistencies or other problems. Given the synthesized and pre-cleaned nature of the dataset there are likely few issues, but the checks will take place nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021fafd-be87-49cf-8efa-d398530ca860",
   "metadata": {},
   "source": [
    "The first step in cleaning a timeseries dataset is calculating the date range for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6db8e5e1-3c02-42a5-8e76-31533353391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01\n",
      "2019-12-31\n",
      "DatetimeIndex(['2010-01-01', '2010-01-02', '2010-01-03', '2010-01-04',\n",
      "               '2010-01-05', '2010-01-06', '2010-01-07', '2010-01-08',\n",
      "               '2010-01-09', '2010-01-10',\n",
      "               ...\n",
      "               '2019-12-22', '2019-12-23', '2019-12-24', '2019-12-25',\n",
      "               '2019-12-26', '2019-12-27', '2019-12-28', '2019-12-29',\n",
      "               '2019-12-30', '2019-12-31'],\n",
      "              dtype='datetime64[ns]', length=3652, freq='D')\n"
     ]
    }
   ],
   "source": [
    "# Finding and printing the first day of the dataset\n",
    "first_day = time_df.index.min()\n",
    "print(first_day)\n",
    "\n",
    "# Finding and printing the last day of the dataset\n",
    "last_day = time_df.index.max()\n",
    "print(last_day)\n",
    "\n",
    "# Finding and printing the date range for the dataset\n",
    "full_range = pd.date_range(start=first_day, end=last_day, freq=\"D\")\n",
    "print(full_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33509c4-89af-4ff3-be63-7177bd6802ef",
   "metadata": {},
   "source": [
    "Now the date range of the dataset can be checked against the index of the dataframe, these values should have no differences, meaning that every day within the date range is present in the index and date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ca7d5d9-c790-42fc-a391-9829649801d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex([], dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_range.difference(time_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab0b9-4099-42f1-9223-6e7e443ceaea",
   "metadata": {},
   "source": [
    "The check was successful and found no evidence of missing days. The only remaining step for the cleaning is checking for null values which is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d25fea3-6ffe-443c-93f7-234f255c5346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total SRs    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78590c1-0b8b-4615-bcbe-a1f58d6293ec",
   "metadata": {},
   "source": [
    "There were no null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e4db2-6916-44fd-9935-50483af74d80",
   "metadata": {},
   "source": [
    "## Part 6 - Conversion to CSV\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401fcc-cdf1-44e5-89ca-7cc1a0ed10e4",
   "metadata": {},
   "source": [
    "Now that all of the necessary cleaning and formatting for this dataset is complete the dataset will be turned into a .csv file for use in other notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5eafa8-6000-4274-8023-5f67a43810c3",
   "metadata": {},
   "source": [
    "### Final dimensions check\n",
    "_____________________________\n",
    "Now the dimensions and first few rows will be checked to verify everything is as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6eb93b1c-dfd6-42b5-82c3-6152d86d8a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total SRs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Created Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>2782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>3692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>7977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>7119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Total SRs\n",
       "Created Date           \n",
       "2010-01-01         2782\n",
       "2010-01-02         3692\n",
       "2010-01-03         5426\n",
       "2010-01-04         7977\n",
       "2010-01-05         7119"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first five rows of the new dataframe\n",
    "time_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c642afc5-a009-4b87-b7ba-65db62185a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3652 entries, 2010-01-01 to 2019-12-31\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   Total SRs  3652 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 57.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Displaying the columns and datatypes of the final dataframe\n",
    "time_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6369404c-76d5-437d-baa2-01e3688943ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3652, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the final dataframe\n",
    "time_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027d199-83ab-4045-b8f9-7d6891eb8bfd",
   "metadata": {},
   "source": [
    "The final step in this notebook is converting the final dataframe into a .csv file for use in the upcoming EDA and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71fb9210-2339-4be4-91e7-17d003b8edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.006004\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "# Converting the dataframe into a .csv file\n",
    "time_df.to_csv('311 Data/nyc_timeseries_311.csv')\n",
    "finish = datetime.now()\n",
    "duration = finish - start\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0c58fcd-99af-451b-b844-29dc9df180e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:37:19.954688\n"
     ]
    }
   ],
   "source": [
    "# Calculating and displaying the notebook runtime\n",
    "e_end = datetime.now()\n",
    "notebook_runtime = e_end - e_start\n",
    "print(notebook_runtime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
